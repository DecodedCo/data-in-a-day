---
title: "Data in a day"
output: html_notebook
---

```{r}
library("rpart")
library("rpart.plot")
library("ggplot2")
```

## Source the data
```{r}
df <- read.csv("mushrooms.csv")
```


## Explore and transform the data
```{r}
head(df)
```

```{r}
summary(df)
```

Let's try and visualise this data with the help of https://www.kaggle.com/shwetp/caret-ggplot2
```{r}
ggplot(df, aes(`odor`, fill = `class`))
```


## Building a model
Since we don't know what will best predict edible or not, let's build a decision tree based on all the features ( the dot after ~ tells rpart to use all features)
```{r}
decisionTree <- rpart(class ~., data=df, method="class")
```


## Evaluate your model
There is a lot of information produced about the tree
```{r}
summary(decisionTree)
```

But it is nice to see it
```{r}
rpart.plot(decisionTree)
```

It is unfortuante that we can't smell a picture! Can we predict without the smell?
```{r}
df$odor <- NULL
decisionTree <- rpart(class ~ ., data=df, method="class")
rpart.plot(decisionTree)
```

Quantitatively, how good are the predictions?
```{r}
prediction <- predict(decisionTree, df, type="class")
correct <- prediction == df$class
data.frame(df$class, prediction, correct)
```

Let's look at the proportion of predictions we got right
```{r}
print(sum(correct))
print(nrow(df))
print(sum(correct)/nrow(df))
```

## Avoiding overfitting
Split data into test (30%) and train (70%)
```{r}
set.seed(100)  # setting seed to reproduce results of random sampling
trainingRowIndex <- sample(1:nrow(df), 0.7*nrow(df))  # row indices for training data
trainingData <- df[trainingRowIndex, ]  # model training data
testData  <- df[-trainingRowIndex, ]   # test data
```

Re run model with training data
```{r}
decisionTree2 <- rpart(class ~., data=trainingData, method="class")
rpart.plot(decisionTree2)
```

Look at predictions using our test data
```{r}
prediction2 <- predict(decisionTree2, testData, type="class")
sum(prediction2 == testData$class)/nrow(testData)
```
